{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyoyN5UVT1qg",
        "outputId": "75c437f2-fc17-4279-bdc1-07e5da6fd58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Collecting bitsandbytes-cuda112\n",
            "  Downloading bitsandbytes_cuda112-0.26.0.post2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes_cuda112-0.26.0.post2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes-cuda112, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1 bitsandbytes-cuda112-0.26.0.post2\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!pip install bitsandbytes-cuda112 bitsandbytes # based on nvcc --version output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69wLaCndVKnO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('dicta-il/dictalm-7b-instruct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxz39qO0R4qy"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('dicta-il/dictalm-7b-instruct', quantization_config=quantization_config, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lFh_rh-UqwL",
        "outputId": "56f7fcb2-8660-4c51-8b03-7e7ecf057e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['מה אתה חושב על המצב במדינת ישראל:\\n\\nאני חושב שאנחנו צריכים לנקוט עמדה חזקה יותר לגבי מה שקורה כאן. אני חושב שחשוב לנקוט עמדה חזקה יותר כדי לתרום לדיון.\\n']\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    prompt = 'מה אתה חושב על המצב במדינת ישראל:\\n'\n",
        "    kwargs = dict(\n",
        "        inputs=tokenizer(prompt, return_tensors='pt').input_ids.to(model.device),\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.75,\n",
        "        max_length=100,\n",
        "        min_new_tokens=5\n",
        "    )\n",
        "\n",
        "    print(tokenizer.batch_decode(model.generate(**kwargs), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0z4R2E1R5rb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
